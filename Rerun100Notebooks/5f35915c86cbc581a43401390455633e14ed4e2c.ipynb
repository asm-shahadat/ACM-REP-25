{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T06:56:07.569663Z",
     "iopub.status.busy": "2023-10-05T06:56:07.569371Z",
     "iopub.status.idle": "2023-10-05T06:56:37.844296Z"
    },
    "reproducibility": "Did not execute"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/envs/testenv/lib/python3.10/site-packages (23.2.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in /opt/conda/envs/testenv/lib/python3.10/site-packages (0.41.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in /opt/conda/envs/testenv/lib/python3.10/site-packages (68.2.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\r\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/e8/d9/104988573fd2c1acdc64e66883b35fb8ae559310d2d9f77db78bf7de9add/gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from gensim) (1.26.0)\r\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from gensim) (1.11.3)\r\n",
      "Collecting smart-open>=1.8.1 (from gensim)\r\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/26.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/26.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/26.5 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/26.5 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/26.5 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/26.5 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/26.5 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/26.5 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m21.2/26.5 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m24.7/26.5 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: smart-open, gensim\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/conda/envs/testenv/lib/python3.10/site-packages (3.8.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from matplotlib) (1.1.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from matplotlib) (0.12.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from matplotlib) (4.43.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from matplotlib) (1.4.5)\r\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from matplotlib) (1.26.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from matplotlib) (23.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from matplotlib) (10.0.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from matplotlib) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/envs/testenv/lib/python3.10/site-packages (3.8.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click in /opt/conda/envs/testenv/lib/python3.10/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/testenv/lib/python3.10/site-packages (from nltk) (1.3.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from nltk) (2023.10.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/testenv/lib/python3.10/site-packages (from nltk) (4.66.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/envs/testenv/lib/python3.10/site-packages (1.26.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/envs/testenv/lib/python3.10/site-packages (2.1.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from pandas) (1.26.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from pandas) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/testenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading sklearn-0.0.post9.tar.gz (3.6 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \b\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for sklearn (setup.py) ... \u001b[?25l-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \b\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \b|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0.post9-py3-none-any.whl size=2952 sha256=5d96442a18356b285344e7a0bf37e914f6f80be60895e743bd5a526a474cfbaf\r\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/33/a3/d2/092b519e9522b4c91608b7dcec0dd9051fa1bff4c45f4502d1\r\n",
      "Successfully built sklearn\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: sklearn\r\n",
      "Successfully installed sklearn-0.0.post9\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\r\n",
      "^C\r\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Update packages\n",
    "\n",
    "%pip install -U pip wheel setuptools\n",
    "%pip install -U gensim\n",
    "%pip install -U matplotlib\n",
    "%pip install -U nltk\n",
    "%pip install -U numpy\n",
    "%pip install -U pandas\n",
    "%pip install -U sklearn\n",
    "%pip install -U umap-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9raLnNdzCWE"
   },
   "source": [
    "# Cluster Documents\n",
    "\n",
    "Clustering documents in three main steps:\n",
    "\n",
    "1. Cleaning and tokenizing data (lowercasing text, removing non-alphanumeric characters, or stemming words).\n",
    "1. Generating vector representations of the documents mapping words into numerical vectors and aggregate them. Usually done via word-embedding.\n",
    "1. Applying a clustering algorithm on the document vectors (K-means, DBSCAN, or Hierarchical Clustering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('drive')\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZxWiiwiy5Vo",
    "outputId": "65273990-16b0-4af2-d6bb-15f4b29fde16"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import umap\n",
    "\n",
    "SEED = 42\n",
    "random.seed(42)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "nrp2z_WRD2LD"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/posts.csv')\n",
    "df = pd.read_csv('../data/posts_cleaned.csv')\n",
    "\n",
    "tokenized_docs = df['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "UQvo40YolXNH"
   },
   "outputs": [],
   "source": [
    "from numpy.lib.function_base import vectorize\n",
    "\n",
    "VECTOR_SIZE=300\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_docs, \n",
    "    vector_size=VECTOR_SIZE, \n",
    "    workers=os.cpu_count(),\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oA66OtiWpHT6",
    "outputId": "9decdaa2-63b0-40ed-b459-4fef85198c0a",
    "reproducibility": "Did not execute"
   },
   "outputs": [],
   "source": [
    "# for kw in ['musk', 'obama', 'trump', 'putin', 'zelensky', 'macron']:\n",
    "# for kw in ['covid', 'vax', 'novax', 'greenpass', 'trump', 'macron', 'mattarella', 'draghi', 'vaccino', 'putin', 'ucraina']:\n",
    "for kw in ['covid', 'vax', 'novax', 'greenpass', 'trump', 'macron', 'mattarell', 'drag', 'vaccin', 'putin', 'ucrain']:\n",
    "    try:\n",
    "        most_similar_words = model.wv.most_similar(kw, topn=5)\n",
    "    except KeyError:\n",
    "        most_similar_words = []\n",
    "    print(kw, [sw[0] for sw in most_similar_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Bis6KB4ivHZS"
   },
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    # the model might not know about a new word\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "\n",
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zt3SmLD93_XT",
    "outputId": "f4d66055-a8ab-46de-aa48-711feb0e88d8",
    "reproducibility": "Did not execute"
   },
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(X, k, mb, print_silhouette_values=False, curr_best=None):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    s_score = silhouette_score(X, km.labels_)\n",
    "\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {s_score:0.4f}\")\n",
    "    print(f\"Inertia: {km.inertia_}\")\n",
    "    \n",
    "    s_values = []\n",
    "    if curr_best is not None and curr_best < s_score:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            s_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        s_values = sorted(\n",
    "            s_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        if print_silhouette_values:\n",
    "            print(f\"Silhouette values:\")\n",
    "            for s in s_values:\n",
    "                print(\n",
    "                    f\"\\tCluster {s[0]}: Size:{s[1]} - Avg:{s[2]:.2f} - Min:{s[3]:.2f} - Max: {s[4]:.2f}\"\n",
    "                )\n",
    "\n",
    "    return km, km.labels_, s_score, s_values\n",
    "\n",
    "\n",
    "# Search for the best k value for clustering\n",
    "min_k_cluster_size = 5\n",
    "max_k_cluster_size = 50\n",
    "max_patience = 10\n",
    "mini_batch_size = 256*os.cpu_count()\n",
    "\n",
    "k = None\n",
    "best_k = None\n",
    "best_s_coeff = -1\n",
    "best_s_values = []\n",
    "best_clustering = None\n",
    "best_cluster_labels = None\n",
    "patience = 0\n",
    "verbose = False\n",
    "history = []\n",
    "\n",
    "\n",
    "for k in range(min_k_cluster_size, max_k_cluster_size+1):\n",
    "    clustering, cluster_labels, s_coeff, s_values = mbkmeans_clusters(\n",
    "        X=vectorized_docs,\n",
    "        k=k,\n",
    "        mb=mini_batch_size,\n",
    "        print_silhouette_values=verbose,\n",
    "        curr_best=best_s_coeff\n",
    "    )\n",
    "    history.append((k, s_coeff))\n",
    "\n",
    "    if s_coeff > best_s_coeff:\n",
    "        best_k = k\n",
    "        best_clustering = clustering\n",
    "        best_cluster_labels = cluster_labels\n",
    "        best_s_coeff = s_coeff\n",
    "        best_s_values = s_values\n",
    "        patience = 0\n",
    "        print(\"New best found!\", f'{best_s_coeff:0.4f}', f'({best_k})')\n",
    "    else:\n",
    "        if patience >= max_patience:\n",
    "            k = best_k\n",
    "            clustering = best_clustering\n",
    "            cluster_labels = best_cluster_labels\n",
    "            s_coeff = best_s_coeff\n",
    "            s_values = best_s_values\n",
    "            break\n",
    "        patience += 1\n",
    "        print(\"Patience:\", patience, '/', max_patience, \"|\", \"Current best:\", f'{best_s_coeff:0.4f}', f'({best_k})')\n",
    "    print()\n",
    "\n",
    "plt.plot(\n",
    "    [x for x, y in history],\n",
    "    [y for x, y in history],\n",
    ")\n",
    "plt.title('Silhouette coefficient')\n",
    "plt.xlabel('# Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G74tI6CXC4Lm",
    "outputId": "1e654f53-c919-40d0-a47c-901920cb0733",
    "reproducibility": "Did not execute"
   },
   "outputs": [],
   "source": [
    "print(\"Clusters:\", k)\n",
    "print(\"Silhoutte coeff:\", s_coeff)\n",
    "print(\"Silhoutte values:\")\n",
    "for s in s_values:\n",
    "    print(\n",
    "        f\"\\tCluster {s[0]}:\\tSize:{s[1]} - Avg:{s[2]:.2f} - Min:{s[3]:.2f} - Max: {s[4]:.2f}\"\n",
    "    )\n",
    "print()\n",
    "\n",
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i, *_ in s_values:\n",
    "    tokens_per_cluster = []\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster.append(t[0])\n",
    "    tokens_per_cluster = ', '.join(tokens_per_cluster)\n",
    "    print(f\"\\tCluster {i}:\\t{tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOh4MUiQ_VUE",
    "outputId": "59fa9e91-084e-4bfd-9b92-c454e5f1a855",
    "reproducibility": "Did not execute"
   },
   "outputs": [],
   "source": [
    "def inspect_cluster(c_id, clustering, docs, vectorized_docs, num=5):\n",
    "    most_representative_docs = np.argsort(\n",
    "        np.linalg.norm(vectorized_docs - clustering.cluster_centers_[c_id], axis=1)\n",
    "    )\n",
    "    print(\"Cluster\", c_id, \"#\"*80)\n",
    "    for i, d in enumerate(most_representative_docs[:num]):\n",
    "        print(f'({i+1})')\n",
    "        print(docs[d])\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "for i, *_ in s_values[:3]:\n",
    "    inspect_cluster(i, clustering, docs, vectorized_docs, num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_neighbors=50  # balance preservation of local vs global structure (lower->local, higher->global)\n",
    "min_dist=1  # controls how tightly UMAP is allowed to pack points together (0-1)\n",
    "\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings_2d = umap.UMAP(\n",
    "    n_neighbors=n_neighbors,\n",
    "    n_components=2,\n",
    "    min_dist=min_dist,\n",
    "    metric='cosine',  # ignore magnitude of the document vectors\n",
    "    random_state=SEED,\n",
    ").fit_transform(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l_Wkp7c-O1xg",
    "outputId": "2c9e5610-ac18-439a-c176-75716ade8285",
    "reproducibility": "Did not execute"
   },
   "outputs": [],
   "source": [
    "scatter_2d = plt.scatter(\n",
    "    umap_embeddings_2d[:, 0],\n",
    "    umap_embeddings_2d[:, 1],\n",
    "    s=0.25,\n",
    "    c=cluster_labels,\n",
    "    cmap='rainbow',\n",
    ")\n",
    "plt.legend(*(scatter_2d.legend_elements()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings_3d = umap.UMAP(\n",
    "    n_neighbors=n_neighbors,\n",
    "    n_components=3,\n",
    "    min_dist=min_dist,\n",
    "    metric='cosine',  # ignore magnitude of the document vectors\n",
    "    random_state=SEED,\n",
    ").fit_transform(vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "reproducibility": "Did not execute"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "scatter_3d = ax.scatter3D(\n",
    "    umap_embeddings_3d[:, 0],\n",
    "    umap_embeddings_3d[:, 1],\n",
    "    umap_embeddings_3d[:, 2],\n",
    "    s=0.25,\n",
    "    c=cluster_labels,\n",
    "    cmap='rainbow',\n",
    ")\n",
    "plt.legend(*(scatter_3d.legend_elements()))\n",
    "ax.xaxis.set_pane_color((0.0, 0.0, 0.0, 0.0))\n",
    "ax.yaxis.set_pane_color((0.0, 0.0, 0.0, 0.0))\n",
    "ax.zaxis.set_pane_color((0.0, 0.0, 0.0, 0.0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a57891e84cad8eebd13a283be4e86d5eacb9104338807a990cd6282d68299c11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
